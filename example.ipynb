{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import DQN\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "names=[] # keep track of names for plotting at the end\n",
    "\n",
    "\n",
    "# basic parameters\n",
    "p = dict(\n",
    "    env_name =              'BreakoutNoFrameskip-v4', # use NoFrameskip-v4 version\n",
    "    log_dir=                'SI_tests',\n",
    "    overwrite_previous=     False,\n",
    "    asynchronous=           False,\n",
    "    \n",
    "    doubleQ=                False,\n",
    "    dueling=                False,\n",
    "    noisy_linear=           False,\n",
    "    categorical_DQN=        False,\n",
    "    prioritized_replay=     False,\n",
    "    n_step_learning=        False,\n",
    "    group_training_losses = True,\n",
    "    data_plotting =         True,\n",
    "    \n",
    "    screen_size=        42,\n",
    "    trailing_avg_trail= 40,\n",
    "    eval_interval=      100_000,\n",
    "    max_steps=          15_000_000, \n",
    "    record_interval=    5_000_000, \n",
    "    n_games_per_eval=   10,\n",
    "    n_envs=             20,\n",
    "    pbar_update_interval= 800,\n",
    "    seed=               42\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:  0%|                                |800/15,000,000[t:00:00/31:36], eps=0, ev_avg=0.0, tr_avg=0.0, rate=0.0 stp/s"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DQN' object has no attribute 'policy_updater'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m p\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m      2\u001b[0m     name\u001b[38;5;241m=\u001b[39m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDDQN\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      3\u001b[0m     note\u001b[38;5;241m=\u001b[39m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'''\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_envs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vectorized environments. \u001b[39m\u001b[38;5;124m'''\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     n_step_learning\u001b[38;5;241m=\u001b[39m    \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     12\u001b[0m dqn \u001b[38;5;241m=\u001b[39m DQN(p)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m names\u001b[38;5;241m.\u001b[39mappend(dqn\u001b[38;5;241m.\u001b[39mfilepaths\u001b[38;5;241m.\u001b[39msub_dir)\n",
      "File \u001b[0;32m~/SynologyDrive/RDQN-vectorized/dqn_agent.py:209\u001b[0m, in \u001b[0;36mDQN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# 2. Update the policy network\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_update_interval_adjusted \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 209\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_updater\u001b[49m\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# 3. Update the target network\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m steps \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_update_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DQN' object has no attribute 'policy_updater'"
     ]
    }
   ],
   "source": [
    "p.update(\n",
    "    name=               'DDQN',\n",
    "    note=               f'''{p['n_envs']} vectorized environments. ''',\n",
    "\n",
    "    doubleQ=            True,\n",
    "    dueling=            False,\n",
    "    noisy_linear=       False,\n",
    "    categorical_DQN=    False,\n",
    "    prioritized_replay= False,\n",
    "    n_step_learning=    False,\n",
    "    )\n",
    "dqn = DQN(p)\n",
    "\n",
    "dqn.train()\n",
    "names.append(dqn.filepaths.sub_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Vanilla DQN'''\n",
    "\n",
    "p.update(\n",
    "    name=               'DQN',\n",
    "    doubleQ=            False,\n",
    "    dueling=            False,\n",
    "    noisy_linear=       False,\n",
    "    categorical_DQN=    False,\n",
    "    prioritized_replay= False,\n",
    "    n_step_learning=    False,\n",
    "    )\n",
    "dqn = DQN(p)\n",
    "dqn.train()\n",
    "names.append(dqn.filepaths.sub_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Rainbow DQN'''\n",
    "\n",
    "p.update(\n",
    "    name=               'Rainbow',\n",
    "    doubleQ=            True,\n",
    "    dueling=            True,\n",
    "    noisy_linear=       True,\n",
    "    categorical_DQN=    True,\n",
    "    prioritized_replay= True,\n",
    "    n_step_learning=    True,\n",
    "    )\n",
    "dqn = DQN(p)\n",
    "dqn.train()\n",
    "names.append(dqn.filepaths.sub_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the results in the log folder\n",
    "import importlib, os\n",
    "import modules.utils\n",
    "importlib.reload(modules.utils)\n",
    "plot = modules.utils.plot_multiple_results\n",
    "\n",
    "col = 'trailing_avg'; assert col in ['best_score','eval_avg','trailing_avg','loss']\n",
    "plot( names, 'trailing_avg')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atari_env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
